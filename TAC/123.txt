Lookup Transformations:

Purpose: A lookup transformation queries the source database (e.g., 'Mouse') to verify or fetch additional information.
Process:

The lookup queries a specific table (e.g., config_vehicle) and checks attributes such as the last modified date.
If changes are detected in the source table, this information is appended to the target table, ensuring traceability and data integrity.

Additional Data Enrichment:
Throughout the ETL process, additional information is added to the data to enhance traceability and support troubleshooting. For instance, the system records when a source table was last modified and what changes occurred. This ensures that data marts are built with reliable and accurate information.

By tracking these changes and transformations, the ETL process ensures consistency and accountability. If an issue arises—such as a structural change in the source table—it is immediately identified and logged, allowing for timely resolution.

In the ETL process, it is crucial to track when changes occur in the data. For this reason, we implemented a mechanism that identifies and logs changes effectively. This functionality is always available if needed but is designed to minimize unnecessary overhead.

Target and Metadata Management:
When changes are made to a target object, the metadata for that object must be re-imported. This allows the workflow to recognize the updated structure. If there are discrepancies, such as a new column or altered data type, the system removes the outdated metadata lines and retains only the necessary updates, ensuring that developers can easily remap the changes. This feature functions as a developer aid, providing visibility into the updates without impacting functionality directly.

Data Vault and Data Mart Architecture:
The data vault serves as the central data warehouse, acting as the single source of truth. The ETL workflow typically follows this sequence:

Staging Area: All data from source systems is loaded into the staging area daily. This includes the entire dataset, regardless of whether changes have occurred.
Comparison: The staging data is compared with the existing records in the data vault to identify changes.
Data Vault Loading: Only the changes (e.g., 5% of the total data) are loaded into the data vault, preserving historical integrity and optimizing processing time.
Data Mart Loading: Data marts are created as subsets of the data vault, tailored for specific business needs.

Key Processes:

Full Data Load: Entire tables are loaded into the staging area daily to ensure quick access and avoid constraints on the source systems.
Change Detection: The ETL process identifies differences between the staging data and the data vault, updating only modified or new records in the data vault.
Transformations: Changes are processed with various transformations, ensuring the data is validated and enriched before being loaded into the data vault or data marts.

Why This Approach?
The full table load into the staging area is faster and less resource-intensive than continuously querying the source system. Processing the data within the staging area avoids locking source tables, ensures referential integrity, and allows thorough validation. By isolating changes, the data vault remains efficient and maintains its role as the central data repository.

Efficiency Considerations:
While loading the entire dataset into the staging area takes minimal time (e.g., 5 minutes for 10 million rows), processing the data into the data vault may take longer (e.g., 30 minutes) due to the necessary transformations and validations. However, this approach is crucial for preserving the integrity and reliability of the data without impacting the source systems.

Best Practices:

Isolate Source Data: Never directly modify the source data. Always work within the staging and data vault environments.
Validation and Verification: Use the staging area as a sandbox to verify changes before committing them to the data vault.
Historical Tracking: Maintain historical records in the data vault to provide a comprehensive view of changes over time.

This method ensures a robust, scalable, and secure data pipeline while adhering to best practices for data integrity and system performance.

Once the data source or central repository is identified with certainty, the process involves the following steps:

Initial Validation:

Confirm the integrity of the original data source or central data repository.
Address any faults in the source data.
Data Operations:

Operate on essential and readable data for clarity.
Use a joiner to compare the source and target datasets. Discrepancies are recorded sequentially (e.g., 1, 2, 3, 4), allowing easy tracking and updating.
Data Hub and Satellite Model:

In a vehicle example, the target data originates from fields in the source, which are connected via the Hub and Satellite model. The model organizes names, relationships, and fields into a structured format.
Complexities are managed by breaking them into simpler components, enabling efficient integration.
Virtual Tables and Views:

Views, defined as select statements stored in the database, act as virtual tables. They provide a consolidated perspective for data analysis.
Data is loaded into data marts, where changes are monitored:

If data changes, it is updated or loaded.
If data remains unchanged, it is discarded.
Consolidated Views and Fact Tables:

For specific data mart requirements, consolidated views provide tailored insights. Dimensions and fact tables play complementary roles:

Fact tables store quantitative, objective data relevant to business users.
Dimensions describe entities (e.g., vehicle attributes) and ensure uniformity across the organization.
Application to Business Units:

The data architecture ensures flexibility for various business units. For instance:

Sales data might track which cars are sold to customers.
Operational data could identify which cars are allocated to specific companies or require maintenance.
Dynamic Relationships:

Fact tables dynamically link organizational units with key metrics (e.g., sales, maintenance).
Dimensions provide descriptive context for interpreting these metrics.
Definitional Clarity:

Definitions and methodologies, such as Kimball’s, underpin the organization of data. While fact tables focus on numerical outcomes, dimensions describe the associated context. It is important to revisit and clarify definitions when necessary for accurate implementation.

Defining Dimensions and Facts:

Dimensions describe the attributes of facts. For example:

A customer dimension may classify customers as "paying" or "driving."
Facts determine specific characteristics, such as whether a customer is paying or driving. This distinction ensures consistency in actions, such as targeting only paying customers for discounts.

Consistency in Definitions:

Maintaining a consistent definition of facts and dimensions is critical. Debates over semantics can arise but are secondary to clarity and consistency in implementation.

Data Flow:

Data Vault to Data Mart:

Views are often used to process logic from the Data Vault to the Data Mart but not in all cases.
Special cases might involve directly staging data without views, such as temporary tables used for intermediate processing.
Staging and Data Mart Rules:

Staging tables are exclusively for developers and not accessible to end users.
End users interact only with Data Marts, which are tailored for specific use cases.
Special Cases:

When users require direct data access, a separate Data Mart can be created, ensuring clear boundaries between different user groups and their data needs.
Avoid sharing Data Marts between users with conflicting requirements to prevent overlap or inconsistencies. Instead, create distinct objects for each group.

Relational Nature of Data Marts:

Data Marts are typically structured as relational databases, not monolithic tables.
For example, reporting systems may use a collection of relational objects rather than a single, oversized table with endless rows.

Key Takeaways:

Staging data is for internal use only and acts as a preparatory step.
Data Marts are the final product for end users, designed to meet their specific needs.
In cases of unique requirements, new Data Marts or copies of existing ones can be created to ensure seamless access without conflicts.

Supporting Business Needs:

The data structure should align with the business requirements:

If a single large table with all attributes better suits the business, create that.
If a star schema with facts and dimensions is more appropriate, design based on that model.

Data Mart Creation:

Each business unit or partner should have a dedicated Data Mart tailored to their needs:

For example, if Sales requests a Data Mart for deployed cars, create one specific to their requirements.
If Maintenance requests similar data but with different usage (e.g., used cars), create a separate Data Mart to prevent conflicts.
Avoid consolidating disparate business needs into a single Data Mart to maintain flexibility and avoid constraints.

Accessing Data Marts:

Technical Users (e.g., Analysts):

SQL-savvy users can access Data Marts using tools like SQL Developer or similar platforms.
They receive credentials and specific roles granting them access to the necessary objects within the database.
Non-Technical Users (e.g., Business Users):

Business users who are not familiar with SQL interact through pre-built reports.
Data management teams translate business needs into reports, publish them to the appropriate environment, and maintain user access.

Data Access Management:

Role Assignments:

Role-based access is configured in tools like Oracle SQL Developer.
Analysts may have access to specific areas, such as staging, Data Vault, and Data Marts.
Business users may have limited access, such as viewing only specific Data Marts or reports.
Role Management Process:

Administrators create user accounts, assign roles, and define access permissions.
Access to sensitive or restricted Data Marts, like IFRS 9, is managed via specific role assignments.

Tools and Workflow:

SQL Developer is commonly used for role management and database access configuration.
Other tools, such as SSMS, may also be used depending on the environment and requirements.

Best Practices:

Create separate Data Marts for distinct business needs to prevent overlap and conflicts.
Ensure clear role definitions and access limitations to maintain data integrity and security.
Provide technical users with tools for direct access and empower non-technical users through user-friendly reports.

Database Tools and Connectivity:

Tools like DBeaver are open-source and can connect to a wide range of databases, including Oracle, Microsoft SQL Server, PostgreSQL, MySQL, MongoDB, and others.
Although shortcuts and configurations may vary across databases, DBeaver provides a unified interface to manage and query them.

Data Vault Structure:

The database comprises different layers: Staging, Data Vault, and Data Marts. Each layer serves a specific purpose:

Staging Area: Temporary storage for incoming data. Data is truncated and reloaded daily.
Data Vault: Centralized repository with Hubs, Links, and Satellites.
Data Marts: Final layer designed for end-user reporting and analysis.

Staging Area Practices:

Tables in the staging area follow a strict naming convention:

Prefixes (e.g., SDT) indicate the source system or purpose.
This helps avoid discrepancies caused by similar metadata, file types, or extension types.
Staging tables are truncated and reloaded daily to ensure data freshness.

Data Vault Design Principles:

Hubs:

Contain unique identifiers (business keys) for entities.
Designed to be as compact as possible for performance reasons.
Satellites:

Store descriptive attributes of entities.
Two types:

Regular Satellites: Contain descriptive fields like attributes and descriptions.
Special Satellites (S-BLOCKS): Store business keys and alternative keys (e.g., car registration number, license plate). These connect to the Hub and provide a one-to-one relationship.
Links:

Establish relationships between Hubs.

Data Mart Access:

Access Control:

End users interact with Data Marts, not the staging area or Data Vault.
Permissions are role-based, managed via tools like Oracle SQL Developer.
Users can query data or access pre-built reports, depending on their technical expertise.
User Types:

Analysts: Use tools like SQL Developer for direct database access.
Business Users: Access reports created by data management teams through BI tools.

Data Management Workflow:

Unique Data Marts are created for different business units to avoid conflicts:

For example, Sales might have a Data Mart for deployed cars, while Maintenance might have a separate one for similar data but different use cases.
Shared Data Marts are avoided to ensure independent updates and prevent disruptions.

Data Processing Logic:

Metadata (e.g., table versions and identifiers) is tracked and stored in the Data Vault.
Regular transformations ensure data accuracy, including:

Truncating staging tables during updates.
Using unique identifiers to maintain consistency.

Query Permissions and Adjustments:

Permissions are assigned based on roles:

Analysts and advanced users may have query rights to staging and Data Vault areas.
Business users have limited access to Data Marts or reports only.
If query rights are missing, permissions are updated as required.

Scripts and Automation:

Scripts manage the creation of objects like Hubs, Links, and Satellites.
Example scenarios:

If a mandatory field is missing, a script ensures the relation is established with default values or a placeholder.
ETL processes attempt to resolve relationships automatically or flag unresolved IDs for review.

Best Practices Summary:

Maintain a clear separation between Staging, Data Vault, and Data Marts.
Use standardized naming conventions to avoid conflicts.
Assign user roles carefully to manage access effectively.
Design Hubs and Satellites to balance performance and flexibility.

Identifying Errors in Dimensions:

A recurring issue occurs when data is missing from a dimension:

For example, if a fact points to a car license plate but no corresponding vehicle exists in the dimension, it often indicates the dimension was not loaded correctly.
A high number of -2 records in a table or fact highlights this issue, commonly due to incomplete or missing data in the dimension.
In cases involving multiple systems (e.g., HyDRA and IFRS), such discrepancies can arise because certain data, like a license plate, is not yet synchronized or balanced.

Handling Time Segments and Versions:

When creating data segments, maintain a continuous timeline:

Use a Common Table Expression (CTE) to manage records efficiently.
For each data change, only the start date is captured for the new version, while the previous version's end date is updated.
If no new version exists, the last version's end date remains infinite (Eternity).
This approach allows flexibility by:

Supporting multiple versions of data without imposing an end-date restriction.
Ensuring new versions can seamlessly replace the current version without breaking the timeline.

Current Data Reporting:

The system is designed to report only current data by applying filters for "current" records. Historical or archived data is not included in these reports unless explicitly required.

Using Views for Optimization:

Views are extensively used to simplify querying and improve performance:

From Data Vault to Data Mart, views are created to consolidate and organize data.
Views reduce complexity by condensing large queries and multiple joins into manageable outputs.
Key Benefits of Views:

Prevent direct querying from the Data Warehouse, which could result in large, unwieldy queries.
Simplify maintenance and improve system robustness.

Best Practices for Data Pipeline:

From staging to the Data Vault, direct mappings are used without views for one-to-one transfers.
From the Data Vault to the Data Mart, views are always used to provide a logical layer for querying and reporting.
This approach ensures that data remains organized and scalable, avoiding inefficiencies seen in earlier systems with no intermediary views.

General Observations:

The introduction of smaller, optimized views has significantly improved system robustness and reduced the complexity of the reporting layer.
Earlier methods of direct querying from the Data Warehouse led to inefficiencies and performance issues.

Next Steps:

Digest the provided information, study the details, and revisit with follow-up questions for clarification or further discussion.
Future sessions can focus on deeper insights or specific areas of interest.

Error Identification and Resolution:

When encountering an error in the number flow manager or session manager, it's essential to use these tools as indicators of potential issues:

Errors often signify that something went wrong during the data load.
To identify the exact problem, refer to the detailed log files or follow-up mechanisms.
Common Scenarios:

For instance, if a dataset was not loaded correctly one day but resolved the next, the error might lie in the initial file or configuration.
Fallout during data loads can be challenging, especially when user expectations (e.g., number of columns in a CSV file) don't match the actual data structure.

Error Handling Steps:

Validate the Session:

Ensure that all session parameters are configured correctly to handle unexpected inputs (e.g., extra columns in a file).
Check Log Files:

Logs are now maintained for 14 days, allowing for detailed troubleshooting. These logs are reviewed during biweekly meetings to identify persistent issues.
Coordinate with Teams:

Errors reported by teams (e.g., TCS) are typically related to specific data loads or deadlocks. These errors are intercepted, reviewed, and resolved promptly.

Proactive Measures:

Understanding the architecture at a granular level helps propose immediate solutions when needed.
If you're unavailable, documenting the system's processes and known issues ensures others can manage and resolve problems in your absence.

Documentation and Knowledge Sharing:

Proper documentation is crucial:

Record all processes, components, and common troubleshooting steps for the system.
Review and refine documentation collaboratively to ensure accuracy and completeness.
Referencing advice from experts, such as Carlos's top-down and bottom-up approaches, aids in understanding and managing complex systems effectively.

Session and Timing:

Sessions like this are planned to provide clarity on the system architecture, workflows, and troubleshooting processes.
The session adhered to the planned schedule (10:30 AM to 12:00 PM) and was well-aligned with expectations.

Conclusion:

The session was productive and informative, offering valuable insights into the system's operation and error management strategies.
Future steps include:

Documenting the session notes.
Reviewing the documentation collaboratively for further refinement.
Continuing to enhance the error resolution process and system understanding.